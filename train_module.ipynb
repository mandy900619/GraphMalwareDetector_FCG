{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.loader import DataLoader                                                        \n",
    "\n",
    "import timeout_decorator\n",
    "from timeout_decorator.timeout_decorator import TimeoutError\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          lr=0.01,\n",
    "                                          weight_decay=5e-4)\n",
    "\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 2)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.conv2(h, edge_index)\n",
    "\n",
    "        h = global_add_pool(h, batch)\n",
    "\n",
    "        h = self.lin1(h)\n",
    "        h = self.bn1(h)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2023-11-18\n",
      "Pytorch version: 2.1.0+cu121\n",
      "PyG version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TIMEOUT = 180\n",
    "\n",
    "print(f\"Date: {date.today()}\")\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f\"PyG version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_timer(some_function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = some_function(*args, **kwargs)\n",
    "        end = time()-t1\n",
    "        return result, end\n",
    "    return wrapper\n",
    "\n",
    "@function_timer\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:                             # Iterate in batches over the training dataset. \n",
    "        data.to(device)                                   # Train the data if gpu is available\n",
    "        optimizer.zero_grad()                             # Clear gradients.    \n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)                     # Compute the loss.\n",
    "        # print(help(loss))\n",
    "        loss.backward(retain_graph=True)                                   # Derive gradients.\n",
    "        optimizer.step()                                  # Update parameters based on gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for data in loader:                                   # Iterate in batches over the training/test dataset.\n",
    "        data.to(device)                                   # Train the data if gpu is available\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Predict the outcome by trained model\n",
    "        pred = out.argmax(dim=1)                          # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())            # Check against ground-truth labels.\n",
    "        loss += criterion(out, data.y).item()             # Get the loss accumulated of each data sample\n",
    "        for i in range(len(pred)):\n",
    "            if (pred[i] == 1):\n",
    "                if (data.y[i] ==1):\n",
    "                    TP += 1\n",
    "                elif (data.y[i] == 0):\n",
    "                    FP += 1\n",
    "            elif (pred[i] == 0):\n",
    "                if (data.y[i] == 0):\n",
    "                    TN += 1\n",
    "                elif (data.y[i] == 1):\n",
    "                    FN += 1\n",
    "        \n",
    "    acc = correct / len(loader.dataset)                   # Get the accuracy\n",
    "    avg_loss = loss / len(loader.dataset)                 # Get the average loss\n",
    "    Prec = TP / (TP + FP)                                 # Get the precision\n",
    "    Recall = TP / (TP + FN)                               # Get the Recall\n",
    "    F1 = 2 * (Prec * Recall) / (Prec + Recall)            # Get the F1_score   \n",
    "    return (acc, avg_loss, Prec, Recall, F1)              # Return the accuracy and average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malware:  (18717, 3)\n",
      "benignware:  (17905, 3)\n"
     ]
    }
   ],
   "source": [
    "datasetPath = \"/home/mandy900619/Documents/detectors/Graph_detector/dataset.csv\"\n",
    "sampleDir = \"/home/mandy900619/Documents/detectors/Graph_detector/graph_data/fcg_gpickle/\"\n",
    "\n",
    "dataset = pd.read_csv(datasetPath)\n",
    "data = dataset\n",
    "for row in dataset.iterrows():\n",
    "    fileName = row[1]['filename']\n",
    "    fileprefix = fileName[0:2]\n",
    "    samplePath = sampleDir + fileName + \".gpickle\"\n",
    "    if not os.path.exists(samplePath):\n",
    "        data = data.drop(data[data['filename'] == fileName].index)\n",
    "        continue\n",
    "\n",
    "malwareData = data[data['label'] == \"malware\"]\n",
    "benignData = data[data['label'] == \"benignware\"]\n",
    "\n",
    "print(\"malware: \", malwareData.shape)\n",
    "print(\"benignware: \",benignData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_GE_Data(data_path, ware_type):\n",
    "    num_graph =  len(data_path)\n",
    "    unpacked_list = pd.read_csv(\"/home/mandy900619/Documents/unpacked_list_csv.csv\")\n",
    "    for i, fpath in enumerate(data_path, start = 1):\n",
    "        # fileName = fpath.split(\"/\")[-1]\n",
    "        # if fileName in unpacked_list['filename'].values:\n",
    "        #     if unpacked_list[unpacked_list['filename'] == fileName]['upx_die_detection'].values[0] == 1:\n",
    "        #         continue\n",
    "        print(f'({i}/{num_graph}) Reading file {fpath}', end='\\r')   \n",
    "\n",
    "        try:\n",
    "            with open(fpath, 'rb') as f:\n",
    "                G = pickle.load(f)\n",
    "            for node in G.nodes():\n",
    "                G.nodes[node]['x'] = torch.nan_to_num(G.nodes[node]['x'], nan=0.0)\n",
    "            torch_data = from_networkx(G)\n",
    "            if torch_data.x is None:\n",
    "                # Some file become NoneType after from_network() transformation\n",
    "                nonetype_files.append(fpath)\n",
    "                continue\n",
    "\n",
    "            torch_data.y = ware_type\n",
    "\n",
    "            graphs.append(torch_data)\n",
    "        except TypeError:\n",
    "            error_files.append(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacked_list = pd.read_csv(\"/home/mandy900619/Documents/unpacked_list_csv.csv\")\n",
    "# # malwareData merge with unpacked_list\n",
    "# malwareData = malwareData.merge(unpacked_list, on='filename', how='left')\n",
    "# malwareData = malwareData[malwareData['upx_die_detection'] != 1]\n",
    "# malware_len = len(malwareData)\n",
    "# #resize the benignData to the same size as malwareData\n",
    "# benignData = benignData.sample(malware_len)\n",
    "\n",
    "# print(\"malware: \", malwareData.shape)\n",
    "# print(\"benignware: \", benignData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17905/17905) Reading file /home/mandy900619/Documents/detectors/Graph_detector/graph_data/fcg_gpickle/89aaace40a3890cefc488b4cb0c92e6477a6833ace2ce07f0983dbbeb412b428.gpickle\r"
     ]
    }
   ],
   "source": [
    "malwarePath = [os.path.join(sampleDir, fileName + \".gpickle\") for fileName in malwareData['filename']]\n",
    "benignPath = [os.path.join(sampleDir, fileName + \".gpickle\") for fileName in benignData['filename']]\n",
    "\n",
    "\n",
    "graphs = []\n",
    "nonetype_files = []\n",
    "error_files = []\n",
    "\n",
    "\n",
    "load_GE_Data(malwarePath, 1)\n",
    "load_GE_Data(benignPath, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36622\n"
     ]
    }
   ],
   "source": [
    "print(len(graphs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dataset: <torch_geometric.loader.dataloader.DataLoader object at 0x7f795d643910>:\n",
      "Training Dataset: <torch_geometric.loader.dataloader.DataLoader object at 0x7f795d641000>:\n",
      "============================================================\n",
      "Number of graphs: 229\n",
      "Number of graphs: 58\n",
      "\n",
      "DataBatch(x=[56191, 100], edge_index=[2, 149851], y=[128], batch=[56191], ptr=[129])\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(graphs)\n",
    "percent_set = int(len(graphs)*0.8)\n",
    "train_loader = DataLoader(graphs[:percent_set], batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(graphs[percent_set:], batch_size=128, shuffle=False)\n",
    "print()\n",
    "print(f'Training Dataset: {train_loader}:')\n",
    "print(f'Training Dataset: {test_loader}:')\n",
    "print('='*60)\n",
    "print(f'Number of graphs: {len(train_loader)}')\n",
    "print(f'Number of graphs: {len(test_loader)}')\n",
    "\n",
    "\n",
    "data = next(iter(train_loader))  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3415, -0.1056, -0.4411,  ..., -0.7215,  0.2629,  0.1771],\n",
      "        [ 0.2428,  0.0415, -0.1402,  ..., -0.5225,  0.1133, -0.0481],\n",
      "        [ 0.2913,  0.1258, -0.2201,  ..., -0.4874,  0.1606,  0.0718],\n",
      "        ...,\n",
      "        [ 1.3225,  0.5074, -1.1686,  ...,  0.0454,  0.7342, -0.6467],\n",
      "        [ 0.5753,  0.1870, -0.5511,  ..., -0.2075,  0.3105, -0.2737],\n",
      "        [ 0.8732,  0.0973, -0.7725,  ...,  0.2129,  0.4250, -0.2302]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "SAGE(\n",
      "  (conv1): SAGEConv(100, 64, aggr=mean)\n",
      "  (conv2): SAGEConv(64, 64, aggr=mean)\n",
      "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "device:cuda\n"
     ]
    }
   ],
   "source": [
    "data_sample = train_loader.dataset[0]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "model = SAGE(in_channels=data_sample.num_node_features, hidden_channels=64, out_channels=64)\n",
    "print(data_sample['x'])\n",
    "print(model)\n",
    "model.to(device)\n",
    "print(f'device:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/100)\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 100\n",
    "\n",
    "train_acc = np.zeros(epochs)\n",
    "train_loss = np.zeros(epochs)\n",
    "train_prec = np.zeros(epochs)\n",
    "train_recall = np.zeros(epochs)\n",
    "train_F1 = np.zeros(epochs)\n",
    "val_acc = np.zeros(epochs)\n",
    "val_loss = np.zeros(epochs)\n",
    "val_prec = np.zeros(epochs)\n",
    "val_recall = np.zeros(epochs)\n",
    "val_F1 =np.zeros(epochs)\n",
    "\n",
    "best_val_acc = 0\n",
    "avg_time = 0\n",
    "best_epoch = 0\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = f'Epoch ({epoch + 1}/{epochs})'\n",
    "    print(epoch_start, end='\\r')\n",
    "    # since train() returns nothing, so ignore the return with '_' and fetch the time taken\n",
    "    _, _time = train()\n",
    "    avg_time += _time\n",
    "    # evaluate the training accuracy and validation accuracy after epoch epoch\n",
    "    train_acc[epoch], train_loss[epoch], train_prec[epoch], train_recall[epoch], train_F1[epoch] = test(train_loader)\n",
    "    val_acc[epoch], val_loss[epoch], val_prec[epoch], val_recall[epoch], val_F1[epoch] = test(test_loader)\n",
    "    \n",
    "    if val_acc[epoch]>best_val_acc:\n",
    "        # save the best model according to validation accuracy\n",
    "        best_val_acc = val_acc[epoch]\n",
    "        best_epoch = epoch\n",
    "        torch.save(model, './models/graphSAGE_model_Hash_graph2.pt')\n",
    "    \n",
    "    # if((epoch+1) % 10 == 0):\n",
    "    #     epoch_start = f'Epoch ({epoch + 1}/{epochs})'\n",
    "    #     print(epoch_start, end=' ')\n",
    "    #     print(f'Train Acc: {train_acc[epoch]:.4f}, Train Loss: {train_loss[epoch]:>7.4f}', end=', ')\n",
    "    #     print(f'Val Acc: {val_acc[epoch]:.4f}, Val Loss: {val_loss[epoch]:>7.4f}', end=' -- ')\n",
    "    #     print(f'Training Time: {_time:.2f}s')\n",
    "    epoch_start = f'Epoch ({epoch + 1}/{epochs})'\n",
    "    print(epoch_start, end=' ')\n",
    "    print(f'Train Acc: {train_acc[epoch]:.4f}, Train Loss: {train_loss[epoch]:>7.4f}', end=', ')\n",
    "    print(f'Val Acc: {val_acc[epoch]:.4f}, Val Loss: {val_loss[epoch]:>7.4f}', end=' -- ')\n",
    "    print(f'Training Time: {_time:.2f}s')    \n",
    "print(f'Acc:{val_acc[best_epoch]: .4f}, Precsion:{val_prec[best_epoch]: .4f}, Recall:{val_recall[best_epoch]: .4f}, F1_Score:{val_F1[best_epoch]: .4f}, Avg Time:{avg_time/100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphDetector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
