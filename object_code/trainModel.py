import torch
import torch.nn.functional as F
import torch_geometric
from torch_geometric.nn import SAGEConv
from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout
from torch_geometric.nn import global_mean_pool, global_add_pool
import os
import numpy as np
from datetime import date
import networkx as nx
from pathlib import Path
import pandas as pd
from torch_geometric.utils.convert import from_networkx
from torch_geometric.loader import DataLoader                                                        
import timeout_decorator
from timeout_decorator.timeout_decorator import TimeoutError
from time import time
import pickle


def trainModel(config):

    class SAGE(torch.nn.Module):
        def __init__(self, in_channels, hidden_channels, out_channels):
            super().__init__()
            self.conv1 = SAGEConv(in_channels, hidden_channels)
            self.conv2 = SAGEConv(hidden_channels, out_channels)

            self.optimizer = torch.optim.Adam(self.parameters(),
                                            lr=config.model.learning_rate,
                                            weight_decay=5e-4)

            self.lin1 = Linear(hidden_channels, hidden_channels)
            self.lin2 = Linear(hidden_channels, 2)
            self.bn1 = BatchNorm1d(hidden_channels)

        def forward(self, x, edge_index, batch):
            h = self.conv1(x, edge_index)
            h = self.conv2(h, edge_index)

            h = global_add_pool(h, batch)

            h = self.lin1(h)
            h = self.bn1(h)
            h = h.relu()
            h = self.lin2(h)
            h = F.dropout(h, p=0.5, training=self.training)

            return F.log_softmax(h, dim=1)

    def function_timer(some_function):
        def wrapper(*args, **kwargs):
            t1 = time()
            result = some_function(*args, **kwargs)
            end = time()-t1
            return result, end
        return wrapper

    @function_timer
    def train():
        model.train()

        for data in train_loader:                             # Iterate in batches over the training dataset. 
            data.to(device)                                   # Train the data if gpu is available
            optimizer.zero_grad()                             # Clear gradients.    
            out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.
            loss = criterion(out, data.y)                     # Compute the loss.
            # print(help(loss))
            loss.backward(retain_graph=True)                                   # Derive gradients.
            optimizer.step()                                  # Update parameters based on gradients.

    def test(loader):
        model.eval()

        loss = 0
        correct = 0
        TP = 0
        FP = 0
        TN = 0
        FN = 0
        for data in loader:                                   # Iterate in batches over the training/test dataset.
            data.to(device)                                   # Train the data if gpu is available
            out = model(data.x, data.edge_index, data.batch)  # Predict the outcome by trained model
            pred = out.argmax(dim=1)                          # Use the class with highest probability.
            correct += int((pred == data.y).sum())            # Check against ground-truth labels.
            loss += criterion(out, data.y).item()             # Get the loss accumulated of each data sample
            for i in range(len(pred)):
                if (pred[i] == 1):
                    if (data.y[i] ==1):
                        TP += 1
                    elif (data.y[i] == 0):
                        FP += 1
                elif (pred[i] == 0):
                    if (data.y[i] == 0):
                        TN += 1
                    elif (data.y[i] == 1):
                        FN += 1
            
        acc = correct / len(loader.dataset)                   # Get the accuracy
        avg_loss = loss / len(loader.dataset)                 # Get the average loss
        Prec = TP / (TP + FP)                                 # Get the precision
        Recall = TP / (TP + FN)                               # Get the Recall
        F1 = 2 * (Prec * Recall) / (Prec + Recall)            # Get the F1_score   
        return (acc, avg_loss, Prec, Recall, F1)              # Return the accuracy and average loss


    def load_GE_Data(data_path, ware_type):
        num_graph =  len(data_path)
        for i, fpath in enumerate(data_path, start = 1):
            print(f'({i}/{num_graph}) Reading file {fpath}', end='\r')   
        
            try:
                with open(fpath, 'rb') as f:
                    torch_data = pickle.load(f)
                if torch_data.x is None:
                    # Some file become NoneType after from_network() transformation
                    nonetype_files.append(fpath)
                    continue

                torch_data.y = ware_type

                graphs.append(torch_data)
            except TypeError:
                error_files.append(fpath)
    TIMEOUT = 180
    print(f"Date: {date.today()}")
    print(f"Pytorch version: {torch.__version__}")
    print(f"PyG version: {torch_geometric.__version__}")

    datasetPath = config.folder.dataset + "dataset.csv"
    sampleDir = config.folder.vectorize

    dataset = pd.read_csv(datasetPath)
    data = dataset
    for row in dataset.iterrows():
        fileName = row[1]['filename']
        fileprefix = fileName[0:2]
        samplePath = sampleDir + fileName + ".gpickle"
        if not os.path.exists(samplePath):
            data = data.drop(data[data['filename'] == fileName].index)
            continue

    malwareData = data[data['label'] == "malware"]
    benignData = data[data['label'] == "benignware"]

    print("malware: ", malwareData.shape)
    print("benignware: ",benignData.shape)

    malwarePath = [os.path.join(sampleDir, fileName + ".pickle") for fileName in malwareData['filename']]
    benignPath = [os.path.join(sampleDir, fileName + ".pickle") for fileName in benignData['filename']]


    graphs = []
    nonetype_files = []
    error_files = []

    load_GE_Data(malwarePath, 1)
    load_GE_Data(benignPath, 0)

    print(f"Dataset length: {len(graphs)}")

    np.random.shuffle(graphs)
    percent_set = int(len(graphs)*0.8)
    train_loader = DataLoader(graphs[:percent_set], batch_size=128, shuffle=True)
    test_loader = DataLoader(graphs[percent_set:], batch_size=128, shuffle=False)
    print()
    print(f'Training Dataset: {train_loader}:')
    print(f'Training Dataset: {test_loader}:')
    print('='*50)
    print(f'Number of graphs: {len(train_loader)}')
    print(f'Number of graphs: {len(test_loader)}')
    data = next(iter(train_loader))  # Get the first graph object.
    print()
    print(data)
    print('='*50)

    data_sample = train_loader.dataset[0]
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SAGE(in_channels=data_sample.num_node_features, hidden_channels=64, out_channels=config.model.dimentions)
    print(data_sample['x'])
    print(model)
    model.to(device)
    print(f'device:{device}')


    optimizer = torch.optim.Adam(model.parameters(), lr=config.model.learning_rate, weight_decay=5e-4)
    criterion = torch.nn.CrossEntropyLoss()
    epochs = config.model.epochs

    train_acc = np.zeros(epochs)
    train_loss = np.zeros(epochs)
    train_prec = np.zeros(epochs)
    train_recall = np.zeros(epochs)
    train_F1 = np.zeros(epochs)
    val_acc = np.zeros(epochs)
    val_loss = np.zeros(epochs)
    val_prec = np.zeros(epochs)
    val_recall = np.zeros(epochs)
    val_F1 =np.zeros(epochs)

    best_val_acc = 0
    avg_time = 0
    best_epoch = 0
    modelPath = config.folder.model + config.modelName
    for epoch in range(epochs):
        epoch_start = f'Epoch ({epoch + 1}/{epochs})'
        print(epoch_start, end='\r')
        # since train() returns nothing, so ignore the return with '_' and fetch the time taken
        _, _time = train()
        avg_time += _time
        # evaluate the training accuracy and validation accuracy after epoch epoch
        train_acc[epoch], train_loss[epoch], train_prec[epoch], train_recall[epoch], train_F1[epoch] = test(train_loader)
        val_acc[epoch], val_loss[epoch], val_prec[epoch], val_recall[epoch], val_F1[epoch] = test(test_loader)
        
        if val_acc[epoch]>best_val_acc:
            # save the best model according to validation accuracy
            best_val_acc = val_acc[epoch]
            best_epoch = epoch
            torch.save(model, modelPath)
        
        if((epoch+1) % 10 == 0):
            epoch_start = f'Epoch ({epoch + 1}/{epochs})'
            print(epoch_start, end=' ')
            print(f'Train Acc: {train_acc[epoch]:.4f}, Train Loss: {train_loss[epoch]:>7.4f}', end=', ')
            print(f'Val Acc: {val_acc[epoch]:.4f}, Val Loss: {val_loss[epoch]:>7.4f}', end=' -- ')
            print(f'Training Time: {_time:.2f}s')

    print(f'Acc:{val_acc[best_epoch]: .4f}, Precsion:{val_prec[best_epoch]: .4f}, Recall:{val_recall[best_epoch]: .4f}, F1_Score:{val_F1[best_epoch]: .4f}, Avg Time:{avg_time/100:.2f}')
